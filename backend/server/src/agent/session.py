import logging
logger = logging.getLogger(__name__)
import litellm
import json
import subprocess
import time
import os
import sys
from pathlib import Path
from dotenv import dotenv_values
import asyncio
from uuid_ import uuid_4
from uuid_ import UUid_

from src.agent.workflows.advisor1 import Advisor1
from src.agent.tools import tools, fetch_elements_from_vector_db, get_json_element_by_id_,init_user_database, read_user_data, write_user_data
default_provid_er="openrouter"
default_model="openrouter_scout"


config = dotenv_values(Path(__file__).parent.parent.parent / ".env")
for key, value in config.items():
    os.environ[key] = value
# the reason i have this function here is that mcp functions can call this too

def send(manager,session_id_,msg,name,method_response="request"):
    """
    Send a message to the user via the manager.

    Parameters
    ----------
    manager : Manager
        The manager object responsible for handling message sending.
    session_id_ : str
        The session identifier for the current interaction.
    msg : str
        The message to send to the user.
    name : str
        The identifier for the sender of the message.
    method_response : str, optional
        The method response type. Defaults to 'request'.
    """
    body= {"message": msg,"uuid_":name,"method_response":method_response}
    print("\Sending MSG to user:---------------->\n")
    print(body)
    asyncio.create_task(manager.send_personal_message(session_id_,body)) 


def completion(mcp,msg,modelid_=default_model,args={}):
    """
    Generate a completion response using the specified model and message.

    Parameters
    ----------
    mcp : ModelContextProtocol
        The model context protocol instance used to interact with the models.
    msg : list
        The message(s) to send to the model for generating a response.
    modelid_ : str, optional
        The model identifier to use. Defaults to 'openrouter_scout'.
    args : dict, optional
        Additional arguments to customize the completion request.

    Returns
    -------
    dict
        The completion response generated by the model.
    """

    compl= mcp.completion(
                    messages=msg,
                    model=modelid_,
                    args=args
                )
    return compl
def compl_send(mcp,manager,session_id_,msg,id_,modelid_=default_model,method_response="request",args={}):
    """
    Sends a completion request to the specified model and forwards the response.

    Parameters
    ----------
    mcp : ModelContextProtocol
        The model context protocol instance used to interact with the models.
    manager : Manager
        The manager object responsible for handling message sending.
    session_id_ : str
        The session identifier for the current interaction.
    msg : list
        The message(s) to send to the model for generating a response.
    id_ : str
        The identifier of the sender or recipient of the message.
    modelid_ : str, optional
        The model identifier to use. Defaults to the global default model.
    method_response : str, optional
        The method response type. Defaults to 'request'.
    args : dict, optional
        Additional arguments to customize the completion request.

    Returns
    -------
    dict
        The completion response generated by the model.
    """

    compl= completion(mcp,msg,modelid_,args)
    print("\Sending completion to user:---------------->\n")
    print(compl)
    send(manager,session_id_,compl,id_,method_response)
    return completion  
  



class ModelContextProtocol:
    def error(self,error):
        if(self.manager):
            self.manager.send_personal_message(self.session_id_, {"message": f"Error received: {error}"})
            print(f"Error received: {error}")
        #raise ValueError(error)
    def __init__(self,manager,session_id_):
        self.models = {}
        self.session_id_ = session_id_
        self.manager = manager
        #   ----------> CHECKING IF POStGRES ENV IS SET <----------
        self.POSTGRES_USER=os.environ.get("POSTGRES_USER","stylegen")
        self.POSTGRES_PASSWORD=os.environ.get("POSTGRES_PASSWORD","stylegen")
        self.POSTGRES_DB=os.environ.get("POSTGRES_DB","main")
        if(self.POSTGRES_USER == ""):
            self.error("POSTGRES environment variables not set")
        #   ----------> CHECKING IF PINECONE_API_KEY IS SET <----------
        self.PINECONE_API_KEY = os.environ.get("PINECONE_API_KEY", "")
        if(self.PINECONE_API_KEY == ""):
            self.error("PINECONE_API_KEY environment variable not set")
        self.INDEX_HOST = os.environ.get("INDEX_HOST", "" )
        if self.INDEX_HOST == "":
            self.error("INDEX_HOST environment variable not set")
        self.NAMESPACE = os.environ.get("NAMESPACE", "__default__")
        #   ----------> CHECKING IF OPENROUTER_API_KEY IS SET <----------
        OPENROUTER_API_KEY=os.environ.get("OPENROUTER_API_KEY","")
        OPENROUTER_API_BASE=os.environ.get("OPENROUTER_API_BASE","https://openrouter.ai/api/v1")
        if OPENROUTER_API_KEY != "" and OPENROUTER_API_KEY is not None:
            self.register_provid_er("open_router",OPENROUTER_API_BASE,OPENROUTER_API_KEY,"openrouter")
        #   ----------> CHECKING IF OLLAMA_HOST IS SET <----------
        OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "http://localhost:11434")
        OLLAMA_API_KEY = os.environ.get("OLLAMA_API_KEY", "")
        if OLLAMA_API_KEY != "":
            self.register_provid_er("ollama_local",OLLAMA_HOST,OLLAMA_API_KEY,"ollama")
        #   ----------> CHECKING IF OPENWEBUI_HOST IS SET <----------
        OPENWEBUI_HOST = os.environ.get("OPENWEBUI_HOST", "https://chat.kxsb.org/ollama")
        OPENWEBUI_API_KEY = os.environ.get("OPENWEBUI_API_KEY", "")
        if OPENWEBUI_API_KEY != "":
            self.register_provid_er("openwebui",OPENWEBUI_HOST,OPENWEBUI_API_KEY,"openwebui")
        #   ----------> CHECKING IF GEMINI_HOST IS SET <----------
        GEMINI_HOST = os.environ.get("GEMINI_HOST", "")
        GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "" )
        if GEMINI_API_KEY != "":
            self.register_provid_er("gemini",GEMINI_HOST,GEMINI_API_KEY,"gemini")
    
    
    # this function should get normalised for general use.
    # all those models should not get registered insid_e
    def register_provid_er(self, host_type, host_url, api_key,provid_er, arguments={}):
        """Registers a host after the server has started."""
        print(f"Registering {provid_er} host: {host_url}")
        # registering some models we need later for the agent workflows
        if host_type == "open_router":
            self.models["openrouter_gpt35"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/openai/gpt-3.5-turbo", "tools":False,**arguments} 
            self.models["openrouter_palm2"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/google/palm-2-chat-bison", "tools":False,**arguments}
            self.models["openrouter_llamaguard12b"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/meta-llama/llama-guard-4-12b", "tools":True,**arguments}
            self.models["openrouter_phi4_3"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/microsoft/phi-4-reasoning-plus", "tools":True,**arguments}
            self.models["openrouter_maverick"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/meta-llama/llama-4-maverick", "tools":True,**arguments}
            self.models["openrouter_scout"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/meta-llama/llama-4-scout", "tools":True,**arguments}
            self.models["openrouter_gemini_2.5_pro_p325"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/google/gemini-2.5-pro-preview-03-25", "tools":True,**arguments}
            self.models["openrouter_gemini_2.5_pro_e325"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/google/gemini-2.5-pro-exp-03-25", "tools":True,**arguments}
            self.models["openrouter_claude3.7"]={"api_key": api_key,"provid_er":"openrouter", "base_url": host_url, "model":"openrouter/anthropic/claude-3.7-sonnet", "tools":True,**arguments}
        elif host_type == "ollama_local":
            self.models["ollama_local"]={"api_key": api_key,"provid_er":"ollama", "base_url": host_url, "model":"ollama/gemma3:27b","tools":True, **arguments}
        elif host_type == "openwebui":
            self.models["openwebui"]={"api_key": api_key, "provid_er":"openwebui","base_url": host_url,"model":"ollama/gemma3:27b","tools":True, **arguments}
        elif host_type == "gemini":
            self.models["gemini"]={"api_key": api_key,"provid_er":"gemini", "base_url": host_url, "model": "gemini/gemini-1.5-pro","tools":True,**arguments}
        else:
            raise ValueError("Invalid_ host type. Must be 'ollama_local', 'gemini' or 'openwebui'.")
        try:
                new_host={f"{host_type}":{
                            "litellm_provid_er": provid_er,
                            "max_tokens": 8192,
                            "api_key": api_key,
                                          }}
                print(new_host)
                # we use base_url since api_base seems to be a deprecated argument
                host_type!= "open_router" and litellm.register_model(new_host)
                print(f"Registered host: {host_type}")
        except Exception as e:
            print(f"Failed to register host {provid_er}: {e}")
            sys.exit(1)
   
    def completion(self, messages, model=default_model,ignored_tools=[],args={}):
        """Tests parallel function calling."""
        print(self.models[model])
        modelid_=self.models[model]['model']
        provid_er=self.models[model]['provid_er']
        allow_tools=self.models[model]['tools']
        print("-----------------------------------------")
        print(f"model: {model}")
        
        print(f"baseurl: {self.models[model]['base_url']}")#gemini/gemini-2.0-flash-001
        args={
            "model":modelid_,
            "messages":messages,
            "api_key":self.models[model]["api_key"],
            **args
        }
            # Convert to dictionary
        if(provid_er=="open_router"):
            args["base_url"]=self.models[model]["base_url"] if "base_url" in self.models[model] else None
        if(allow_tools):
            args["tools"] = tools
            args["tool_choice"] = "auto"
            #tools=tools,#[tool for tool in tools if tool['function']['name'] not in ignored_tools], # Filter tools here
        response = litellm.completion(**args,)
        response = dict(response)
        try:
            response_message = response["choices"][0].message.content
            json_response = response_message# json.loads(response_message)    
            print("------------------R-----------------------")
            print("\nLLM Response:\n")
            print(response_message)
            print("------------------R----------------------")
        except Exception as e:
            print("------------------E-----------------------")
            print(e)
            print("------------------E-----------------------")
            return {"error": "Unexpected response format from the model."}
        
        # Step 2: check if the model wanted to call a function
        tool_calls =  response_message.tool_calls if allow_tools  else False
        print("\nLength of tool calls", (tool_calls))
        if tool_calls:
            
            # Step 3: call the function
            # Note: the JSON response may not always be valid_; be sure to handle errors
            available_functions = {
                "get_json_element_by_id_": get_json_element_by_id_,
                "fetch_elements_from_vector_db": fetch_elements_from_vector_db,
                "init_user_database":init_user_database, 
                "read_user_data":read_user_data, 
                "write_user_data":write_user_data
            }  
            messages.append(response_message)  # extend conversation with assistant's reply
            # Step 4: send the info for each function call and function response to the model
            for tool_call in tool_calls:
                function_name = tool_call.function.name
                function_to_call = available_functions[function_name]
                function_args = json.loads(tool_call.function.arguments)
                if function_name == "get_json_element_by_id_":
                    function_response = function_to_call(id_=function_args.get("id_"))
                else:
                    function_response = function_to_call(query=function_args.get("query"))
                messages.append(
                    {
                        "tool_call_id_": tool_call.id_,
                        "role": "tool",
                        "name": function_name,
                        "content": function_response,
                    }
                )  # extend conversation with function response
        # json_response["messages"] = messages
        
        # json_response["step"] = step+1
        return {"response":response_message,"messages":messages}

class Agent():
    def  __init__(self, session,name, modelid_="openrouter_gpt35",args={}, pydantic_response=False):
        self.session=session
        self.id_=(f"{name}_{str(uuid_4())}")
        self.modelid_=modelid_
        self.ready=True
        self.waiting=False
        self.pydantic_response=pydantic_response
        if(pydantic_response):
            self.requiresUserResponse=False# also needs a response field in the pydantic response body
            self.requiesToolResponse=False # also needs a response field in the pydantic response body
            self.pydantic_response={
                "requiresUserResponse": int,
                "requiresToolResponse": int,
                **pydantic_response
            }
        self.args=args
        session.contextRegistry.register_recipient("agent",self.id_)
    
    async def waitTillReady(self):
        while(self.ready == False):
            await asyncio.sleep(1)
            print(f"agent {self.id_} is not ready yet, sleeping")
        print(f"agent {self.id_} is ready")
        self.waiting=False
        self.ready=True
    
    def update(self,msg):
        if(self.pydantic_response):
            self.checkCompletionStatus(msg)
            if(msg.role=="user" and self.requiresUserResponse==True):
                self.requiresUserResponse=False
            else:
                print("agent still waiting for user response")
                return
            if(msg.role=="tool" and self.requiesToolResponse==True):
                self.requiesToolResponse=False
            else:
                print("agent still waiting for tool response")
                return
            
        if(self.ready == False):
            if(self.waiting == True):
                print("agent already busy")
                return
            else:
                self.waiting=True
                self.ready=False
                asyncio.create_task(self.waitTillReady())

    def get_messages(self):
        return self.session.contextRegistry.agentMessages[self.id_]
    def clear_messages(self):
        self.session.contextRegistry.agentMessages[self.id_]=[]
    def checkCompletionStatus(self,msg):
        if(self.pydantic_response):
            if(msg.requiresUserResponse>0.5):
                self.requiresUserResponse=True
            if(msg.requiresToolResponse>0.5):
                self.requiesToolResponse=True
    def send(self,msg):
        method="response"
        send(self.session.manager,self.session.session_id_,msg,self.id_,method_response=method)
    def compl_send(self,msg,model,method_response,args):
        method="request"
        resp= compl_send(self.session.mcp,self.session.manager,self.session.session_id_,msg,model,method_response,args)
        self.session.contextRegistry.agentMessages[self.id_].append(resp)
        self.update(resp)
        return resp
    

class ContextRegistry():
    def __init__(self,session):
        self.session=session
        self.agentMessages={}
        self.toolCalls=[]
        self.userMessages=[]
        self.crewMessages={}
        self.plan=""
        self.important_notes=""
        self.userData={}
    def register_recipient(self,_type,id_):
        if(_type=="agent"):
            self.agentMessages[id_]=[]
        elif(_type=="crew"):
            self.crewMessages[id_]=[]
        else:
            print("unknown recipient type")
    def get_agent(self,id_):
        return self.agentMessages[id_]
    def update_agent(self,id_,msg):
        self.agentMessages[id_].append(msg)
        self.session.agents[id_].update(msg)
    def get_tool(self,id_):
        return self.toolCalls[id_]
    def update_tool(self,id_,msg):
        self.toolCalls[id_].append(msg)
    def get_user(self,id_):
        return self.userMessages[id_]
    def update_user(self,id_,msg):
        self.userMessages[id_].append(msg)
    def get_crew(self,id_):
        return self.crewMessages[id_]
    def update_crew(self,id_,msg):
        self.session.crews[id_].update(msg)
        self.crewMessages[id_].append(msg)

class MessageManager():
    async def listener(self):
        try:
            while True:
                    asyncio.sleep(10)
                    msg= await self.session.websocket.receive_text()
                    print(msg)
                    msg=json.loads(msg)
                    msg=msg.msg
                    method=msg.method
                    role=msg.role
                    content=msg.content
                    if(method=="response"):
                        # Agent or Crew is receiving a Message
                        receipient=msg.receipient
                        if(role=="user"):
                            self.session.contextRegistry.userMessages.append(msg)
                        elif(role=="tool"):
                            self.session.contextRegistry.toolCalls.append(msg)
                        elif(role=="agent"):
                            self.session.contextRegistry.agentMessages.append(msg)
                        elif(role=="crew"):
                            self.session.contextRegistry.crewMessages.append(msg)
                        else:
                            print("unknown role")
                        if(receipient.split("_")[0]=="agent"):
                            receipientObj=self.session.agents[receipient]
                            receipientObj.update(msg)
                        elif(receipient.split("_")[0]=="crew"):
                            receipientObj=self.session.crews[receipient]
                            receipientObj.update(msg)
                        else:
                            print("unknown receipient")
                    # User is updating stuff, or something else happens that is not directly agent related                            
                    elif(method=="updateUserSettings"):
                        self.session.contextRegistry.user_data=msg
                
        except Exception as e:
            print(e)
            asyncio.sleep(1)
    def init(self,session):
        self.session=session
        self.task=asyncio.create_task(self.listener())
        
    def stop(self):
        self.task.cancel()
        self.task=None
        print("listener stopped")

class Session():
    def __init__(self, manager,websocket,session_id_, max_tokens=150,temperature=0.7,max_recursion_depth=10):
        self.mcp = ModelContextProtocol(manager,session_id_)
        self.manager = manager
        self.websocket = websocket
        self.session_id_ = session_id_
        self.models = self.mcp.models
        self.max_recursion_depth = max_recursion_depth
        self.agents={}
        self.crews={}
        self.contextRegistry=ContextRegistry(self)
        self.messageManager=MessageManager()        
        #flow = Advisor1(session=self,mcp=self.mcp,websocket=self.websocket,manager=self.manager,session_id_=self.session_id_)
        





# async def compl_send_await(websocket,mcp,manager,session_id_,msg,modelid_=default_model,method_response="request",args={}):
#     print(msg)
#     question= mcp.completion(
#                     messages=msg,
#                     model=modelid_,
#                     args=args
#                 )
#     name=str(uuid_4())
#     # tell the user you want something
#     print("\nAsking user for responce about:---------------->\n")
    
#     manager.respondMsgs[name]=None
#     question["method"]=method_response
#     asyncio.create_task(manager.send_personal_message(session_id_, {"message": question,"uuid_":name,"method_response":method_response})) 

#     print(f"question: {question} task: {name}")#question)
#     while True:
#         if(manager.respondMsgs[name] is not None):
#             response = manager.respondMsgs[name]
#             print("\nReceived User Response---------------->\n")
#             print(response)
#             break
#         else:
#             print(f"waiting for response of task: {name} - msg: {msg}")
#         time.sleep(10); 
#     return response  
  




# database_manager:
#   role: >
#     {topic} Senior Data Researcher
#   goal: >
#     Find the most relevant items using intelligent vector semantic search
#   backstory: >
#     You're a trained data analyst who accesses the vector database and finds the most relevant {topic} items.
#     You want to help the user finding suiting items and therefore you know how to find the right keywords. 
#     Known for your ability to find the most relevant
#     information and present it in a clear and concise manner.